{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make operating with large and variable data more comfortable a number of special libraries have been developed. One important one is Pandas. Pandas is a big library with statistical tools, convenient data importing functions and tools to filter and extract data.\n",
    "\n",
    "A whole bunch of tutorials for Pandas are available online:\n",
    "https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html<br>\n",
    "or<br>\n",
    "https://dataanalysispython.readthedocs.io/en/latest/index.html<br>\n",
    "The latter beeing my personal favorite<br>\n",
    "\n",
    "We will focus on 3 functions of pandas:\n",
    "1. The convenient way to slice by names\n",
    "2. The import function, that has options for most types of data\n",
    "3. The very simple but powerful plot function\n",
    "\n",
    "Lets look on our Cookbook again, convert it into a DataFrame, the central storage format of Pandas and then instead of using the general print - function use the beautiful printing that is build into Pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Cookbook={'pancakes':{'description':'delicious food prepared fast',\n",
    "                    'time':10.0,'portions':6},\n",
    "        'chocolade icecream':{'description':'my favourite cholate icecream receipe',\n",
    "                    'time':5.0,'portions':3}}\n",
    "#Now we convert it to DataFrame and print it\n",
    "df=pd.DataFrame(Cookbook)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the top horizontal bar (pancakes and chocolade icecream) is called columns and represent data-sets the vertical bar on the left is called index or rows and represents the content of each dataset. \n",
    "\n",
    "For categoric data like this cookbook we could now filter after how long it takes to make a receipe by horizontal (row) slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['time',:]<9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or get the information for one dataset by vertical (column) slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'pancakes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have already used one of the great advantages of Pandas, the name based slicing using the \"locator\"<br>\n",
    "df.loc[\"name of row\",\"name of column\"]<br>\n",
    "As before we can use the \":\" to represent \"from beginning until\" or \"from here until\", or simply \"all\"\n",
    "This is where the 2 (or more!) dimensional access is really useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this was kind of useful for categorical data, but becomes really useful for e.g. spectroscopic data<br>\n",
    "Here we read in the measured absorption of silicon<br> \n",
    "and in the left panel slice only plot the visible range of the measured data<br> \n",
    "while in the right panel the series is plotted as read<br>\n",
    "\n",
    "Important: Both the index and the columns are usually list of strings (text). To really use the clever slicing we need to convert them into floats (third line in code below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data and put the first column as index\n",
    "df_spec=pd.read_csv(\"http://www.jensuhlig.de/Kemm30/Silicon_Absorption.csv\",index_col=0)\n",
    "df_spec.index=df_spec.index.astype(float)#convert the index from the \n",
    "\n",
    "fig,(ax_left,ax_right)=plt.subplots(1,2)  #ignore this for now, we are gone discuss plotting in the next notebook\n",
    "df_spec.plot(ax=ax_right,title='As read')\n",
    "\n",
    "df_sliced=df_spec[400.5:750.5]\n",
    "df_sliced.plot(ax=ax_left,title='Sliced in the visible range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that we sliced from \"400.5\"? by looking into the original dataframe you will notice that the value 400.5 does not exist. The location based slicing can find the nearest position, if the type of the index are numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spec[398:403]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that we used a df_spec[:] 1-dimension, while for the cookbook we used df.loc[:,'pancakes'] 2-dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single column DataFrame is called a series, which is its own class. However also a DataFrame can have only one column. (as df_spec). While this feels like a little bit confusing, this is originating in same difference as numpy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(df),'has dimensions',df.ndim)\n",
    "print(type(df.iloc[:,1]),'has dimensions',df.iloc[:,1].ndim)\n",
    "print(type(df_spec),'has dimensions',df_spec.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also where a lot of the similarities come from. <br>\n",
    "Pandas also has the usual index based slicing like in numpy using the \"index locator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spec.iloc[10:13] # row 10 - 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and all the usual math functions from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('minimum',df_spec.min())\n",
    "print('maximum',df_spec.max())\n",
    "print('average',df_spec.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and all the values can be converted back into numpy arrays (e.g. after importing) by using their \"value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array=df_spec.values\n",
    "index_array=df_spec.index.values\n",
    "columns_array=df_spec.columns.values\n",
    "#print(data_array,index_array,columns_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data in \" http://www.jensuhlig.de/Kemm30/sinc.dat \" using the function df=pd.read_csv() \n",
    "\n",
    "For this you can either open the file directly from the internet or download it into your working directory first. \n",
    "\n",
    "The open the file and note (maybe on a paper) \n",
    "\n",
    "1. if there is any text before the data that you do not want to read\n",
    "2. if there are any headers (text before the data that tells what the columns are)\n",
    "3. what are the separators between the numbers\n",
    "4. which column (if any) would be good to use as index (think x-axis in plot)\n",
    "\n",
    "Play with the options until you get the\n",
    "\n",
    "in the index. the options filename (the internet adress), sep (the separator) and index_col (which column to use for the index) are the ones that you need to take a good look at. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything worked and you named you imported dataframe *df_read* <br>\n",
    "it should look like this if you call *df_read.head()*\n",
    "\n",
    "<div>\n",
    "<img src=\"http://www.jensuhlig.de/Kemm30/sinc_reading.jpg\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "now try what df_read.plot() gives you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you convert the index and the columns to numbers, sorting makes more sense. Compare the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read=df.copy() #to make a deep (real) copy\n",
    "df_read.columns=df_read.columns.values.astype(float)\n",
    "df_read.index=df_read.index.values.astype(float)\n",
    "df_read.sort_index(inplace=True,axis='columns')\n",
    "df_read.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly it is a good idea to give both columns and rows a name, which is then also used during plotting. The y-scale label you typically have to write by hand (tomorrow). Plot and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.index.name='x-value'\n",
    "df_read.columns.name='parameter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce the same plot for \"http://www.jensuhlig.de/Kemm30/sinc_2.dat\". The first row are the headers and should be read! if the data looks strange follow the stepwise check from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start to load and handle many datafiles we should quickly look into how to handle directories. \n",
    "Loading a file from a sub directory needs the filesystem separator, meaning the sign that separates folders and files.\n",
    "\n",
    "If you are working on \"binder\" or \"garm\" then the program of your Jupyter notebook is running on the server. This means that you need to upload your data to this server to be able to use it.\n",
    "\n",
    "Lets assume that you have your datafiles in a (relative) subfolder with the name \"data\" Then during file import you need to replace you filename \"sinc.dat\" with the path to the file<br>\n",
    "under windows this means using the backslash \"data\\sinc.dat\"<br>\n",
    "as however the \"\\\" is a special character we need to use \"\\\\\" to create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='data\\\\sinc.dat' #under windows\n",
    "filename='data/sinc.dat' #under MAC and Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library \"os\" provides some simple tools to make this more comfortable <br>\n",
    "\"os.sep\" provides the platform independent separator (on linux and windows alike) <br>\n",
    "\"os.getcwd()\" read \"get current working directory\" can tell you where you currently are<br>\n",
    "For more complicated path there is the excellent library \"pathlib\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #import the library\n",
    "filename='data' + os.sep + 'sinc.dat'\n",
    "filename=os.sep.join(['data','sinc.dat']) #easy to read and independent of which platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a new folder in your working directory and copy the sinc.dat file into it (using the normal folders of the computer or the upload / new folder functions) . <br>\n",
    "Read and plot this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "The data file \" http://www.jensuhlig.de/Kemm30/APS_Copper_SolarCell.dat \" was measured at a large scale research facility: the \"Advanced light source\" in Chicago and represents X-ray absorption data. Ignore the first 30 rows (and header) and read all columns from the file. Hint the separator \"\\s+\" separate for one or multiple white spaces.\n",
    "\n",
    "Use copy paste to extract the column names (Row 30) from the text file and paste it in your Notebook as a long string. Then use the string method \"split\" to separate the string into a list of column names\n",
    "\n",
    "give this list of names to the use the \"names\" parameter to give the columns in the DataFrame the right name. \n",
    "\n",
    "select the index column by the name \"Energy\" instead of the number.\n",
    "\n",
    "Choose to only read the column \"PR\" (column 14)\n",
    "\n",
    "If you have done the selection during the import then plotting the column \"PR\" vs \"Energy\" is simply\n",
    "df_read.plot() \n",
    "\n",
    "If you import all columns, but want to select only one to plot you would use: df[\"PR\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything worked as expected the plot should look like this:\n",
    "<div>\n",
    "<img src=\"http://www.jensuhlig.de/Kemm30/APS_reading.jpg\" width=\"300\">\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation, this is most likely one of the most difficult files you ever need to read, combining all the techniques you have learned up to now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries are often the easiest way to create and name DataFrames other methods are attaching columns to dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timen=np.arange(0,60,0.1)   # create a time vector\n",
    "dicten={}                   # create an empty dictionary to contain the data\n",
    "for rate in np.arange(1.5,10,1):     # loop over the parameter you want to use and put the current parameter into \"rate\"\n",
    "    y=np.exp(-timen/rate)            # create the vector with the y-values \n",
    "    dicten['%.1f'%rate]=y            # store the value in the dictionary with the parameter as the key \n",
    "df=pd.DataFrame(dicten)              # Now create the dataframe. \n",
    "df.index.name='time [s]'             # give the x-axis a name\n",
    "df.columns.name='rate [mol/s]'       # give the parameter a name\n",
    "df.name='Concentation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the resulting data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For other ways please see the documentation and the cheatsheet. Usuful other methods are to concatenate multiple vectors or to simply calcualte new columns from old columns (the most used method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The manual solution to column reading would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names='N  Epoch  Energy  k  Mono  Seconds  ion1  ion2  ion3  ion4  mud  mud1  mostabc  SYNM  PR  bun-1  bun-1GS  bun1  bun1GS  bun2  bun2GS  DifSYN  Difb_1  Difb1  Difb2  c0o0b0  c0o1b0  c0o2b0  c0o3b0  c0o4b0  c0o5b0  c0o6b0  c0o7b0  c0o8b0  c0o9b0  c0o10b0  c0o11b0  c0o12b0  c0o13b0  c0o14b0  c0o15b0  c0o16b0  c0o17b0  c0o18b0  c0o19b0  c0o20b0  c0o21b0  c1o0b0  c1o1b0  c1o2b0  c1o3b0  c1o4b0  c1o5b0  c1o6b0  c1o7b0  c1o8b0  c1o9b0  c1o10b0  c1o11b0  c1o12b0  c1o13b0  c1o14b0  c1o15b0  c1o16b0  c1o17b0  c1o18b0  c1o19b0  c1o20b0  c1o21b0  c2o0b0  c2o1b0  c2o2b0  c2o3b0  c2o4b0  c2o5b0  c2o6b0  c2o7b0  c2o8b0  c2o9b0  c2o10b0  c2o11b0  c2o12b0  c2o13b0  c2o14b0  c2o15b0  c2o16b0  c2o17b0  c2o18b0  c2o19b0  c2o20b0  c2o21b0  c3o0b0  c3o1b0  c3o2b0  c3o3b0  c3o4b0  c3o5b0  c3o6b0  c3o7b0  c3o8b0  c3o9b0  c3o10b0  c3o11b0  c3o12b0  c3o13b0  c3o14b0  c3o15b0  c3o16b0  c3o17b0  c3o18b0  c3o19b0  c3o20b0  c3o21b0'\n",
    "col_names=col_names.split()\n",
    "print('print the first 8 entries: {}'.format(col_names[:8]))\n",
    "df3=pd.read_csv(\"http://www.jensuhlig.de/Kemm30/APS_Copper_SolarCell.dat\",skiprows=30,sep='\\s+',names=col_names,index_col='Energy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are automatic ways how to do this column extraction:\n",
    "The first method only reads the columns you want and reduces the work, but requires me to count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3a=pd.read_csv(\"http://www.jensuhlig.de/Kemm30/APS_Copper_SolarCell.dat\",skiprows=30,sep='\\s+',names=['Energy','PR'],index_col=0,usecols=[2,14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method uses an escape character to extract the column names automatically and allows then the use of the name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3b=pd.read_csv(\"http://www.jensuhlig.de/Kemm30/APS_Copper_SolarCell.dat\",skiprows=29,escapechar='L',sep='\\s+',index_col=0,usecols=['Energy','PR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Task: \n",
    "\n",
    "Create the a DataFrame with position from -5 cm to 5 cm as the index and in the columns different gaussians bell curves <br>\n",
    "${\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}\\operatorname {exp} \\left(-{\\frac {\\left(x-\\mu \\right)^{2}}{2\\sigma ^{2}}}\\right)$<br>\n",
    "with the same central position (mu=0) and different width sigma (0.5,1,2,3,4)\n",
    "using the simplified plotting from above to show them in the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
